{\rtf1\ansi\ansicpg1252\cocoartf1671\cocoasubrtf600
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 I define the gradients backwards, starting at the cross entropy loss. The gradients are expressed for scalars, from which we can generalise for tensors.\
\\begin\{center\}\
	$\\frac\{\\partial L\}\{\\partial x_\{i\}^\{N\}\} = $ \\\\\
	$\\frac\{\\partial \}\{\\partial x_\{i\}^\{N\}\} -\\Sigma_\{i\}t_\{i\}logx_\{i\}^\{N\}= $ \\\\\
	$\\frac\{t_\{i\}\}\{x_\{i\}^\{N\}\}\
	\
\\end\{center\}\
For the softmax we have:\
\\begin\{center\}\
	$\\frac\{\\partial x^\{N\}\}\{\\partial x!hat!_\{I\}^\{N\}\} = $ \\\\\
\
\\end\{center\}\
For the following layers I denote l<N as h. \
For the leaky Relu:\
\\begin\{center\}\
	$\\frac\{\\partial x^\{h\}\}\{\\partial x!hat!_\{i\}^\{h\}\} = $ \\\\\
	$\\frac\{\\partial\}\{\\partial x!hat!_\{i\}^\{h\}\}max(x!hat!_\{i\}^\{h\}, 0) + a*min(x!hat!_\{i\}^\{h\}, 0 )= $ \\\\\
	$max(1, 0) + a * min(1, 0)\
\
\\end\{center\}\
So the derivative is either 0 or 1.\
\
For the hidden layers we calculate the gradients w.r.t. the input\
\\begin\{center\}\
	$\\frac\{\\partial x!hat!^\{h\}\}\{\\partial x_\{i\}^\{h-1\}\} = $ \\\\\
	$\\frac\{\\partial \}\{\\partial x_\{i-1\}^\{h-1\}\}\\Wx + b = $ \\\\\
	$x_\{i\}^\{h-1\}$\
\
\\end\{center\}\
So the derivative w.r.t. to the input is the input, as this is a linear module.\
\
The derivative w.r.t. the weights is :\
\\begin\{center\}\
	$\\frac\{\\partial x!hat!^\{h\}\}\{\\partial w_\{ij\}^\{h-1\}\} = $ \\\\\
	$\\frac\{\\partial \}\{\\partial w_\{ij\}^\{h-1\}\}\\Wx + b = $ \\\\\
	$x_\{i\}^\{h-1\}$\
\
\\end\{center\}\
So the derivative w.r.t. to the weights is the input neutron activity at the start of the weight-edge, as this is a linear module.\
\
The derivative w.r.t. the bias is \
\
\\begin\{center\}\
	$\\frac\{\\partial x!hat!^\{h\}\}\{\\partial b^\{h-1\}\} = $ \\\\\
	$\\frac\{\\partial \}\{\\partial b^\{h-1\}\}\\Wx + b = $ \\\\\
	$1$\
\
\\end\{center\}\
As the bias is a constant term in the linear equation.\
}